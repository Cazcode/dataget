import os
import shutil
from abc import ABCMeta
from abc import abstractmethod
from abc import abstractproperty
from copy import copy
import pandas as pd


def merge(*datasets, **kwargs):

    if len(datasets) < 2:
        raise Exception("Please merge atleast 2 datasets, got {}".format(len(datasets)))

    # get subsets
    training_sets = datasets |> map$(.training_set) |> list
    test_sets = datasets |> map$(.test_set) |> list

    # load subsets
    for set in training_sets + test_sets:
        set._load_dataframe(**kwargs)

    # get dataframes
    df_train = training_sets |> map$(._dataframe) |> list |> pd.concat$(?, axis = 0)
    df_test = test_sets |> map$(._dataframe) |> list |> pd.concat$(?, axis = 0)

    # get base class copy
    base_dataset = datasets[0] |> copy
    base_dataset.training_set._dataframe = df_train
    base_dataset.test_set._dataframe = df_test

    #reset state
    base_dataset._complete_set = None

    return base_dataset





class DataSet(object):
    __metaclass__ = ABCMeta

    def __init__(self, name, home_path):
        self.name = name
        self.path = os.path.join(home_path, self.name)
        self.training_set = self.training_set_class(self, "training-set")
        self.test_set = self.test_set_class(self, "test-set")

        self._complete_set = None


    @property
    def complete_set(self):
        import pandas as pd

        if self._complete_set is None:
            self.training_set._load_dataframe()
            self.test_set._load_dataframe()

            df_train = self.training_set._dataframe
            df_test = self.test_set._dataframe
            df_complete = pd.concat([df_train, df_test], axis = 0)

            self._complete_set = self.training_set_class(self, "complete-set")
            self._complete_set._dataframe = df_complete

        return self._complete_set


    def make_dirs(self):
        if not os.path.exists(self.path):
            os.makedirs(self.path)

        self.training_set.make_dirs()
        self.test_set.make_dirs()


    def before_op(self, **kwargs):
        pass


    def get(self, download=True, rm=False, rm_compressed=True, process=True, rm_raw=True, **kwargs):
        self.before_op(**kwargs)

        # rm
        if rm:
            self.rm(**kwargs)

        # return if path exists, dataset downloaded already, else create path
        if not self.is_empty():
            return self

        # get data
        if download:
            self.download(**kwargs)

        self.extract(**kwargs)

        # process
        if process:
            self.process(**kwargs)

            if rm_raw:
                self.rm_raw(**kwargs)

        # clean
        if rm_compressed:
            self.rm_compressed(**kwargs)

        return self


    def download(self, rm=False, **kwargs):
        self.before_op(**kwargs)
        print("===DOWNLOAD===")

        # rm
        if rm:
            self.rm(**kwargs)

        if not self.is_empty():
            return self


        self.make_dirs()

        self._download(**kwargs)

        print("")

        return self

    def extract(self, **kwargs):
        self.before_op(**kwargs)

        print("===EXTRACT===")

        self.make_dirs()

        self._extract(**kwargs)

        print("")

        return self

    def rm_compressed(self, **kwargs):
        self.before_op(**kwargs)
        print("===RM-COMPRESSED===")

        self._rm_compressed(**kwargs)

        print("")

        return self

    def process(self, **kwargs):
        self.before_op(**kwargs)

        print("===PROCESS===")

        self._process(**kwargs)

        print("")

        return self

    def rm_raw(self, **kwargs):
        self.before_op(**kwargs)
        print("===RM-RAW===")

        self._rm_raw(**kwargs)

        print("")

        return self


    def rm(self, **kwargs):
        self.before_op(**kwargs)

        if os.path.exists(self.path):
            self.path.split("/") |> .[-1] |> print
            shutil.rmtree(self.path)

        return self

    def rm_subsets(self, **kwargs):
        self.before_op(**kwargs)

        if os.path.exists(self.training_set.path):
            shutil.rmtree(self.training_set.path)

        if os.path.exists(self.test_set.path):
            shutil.rmtree(self.test_set.path)

        return self

    def is_empty(self):
        if not os.path.exists(self.path):
            return True
        else:
            return not os.listdir(self.path)


    @abstractproperty
    def training_set_class(self):
        pass

    @abstractproperty
    def test_set_class(self):
        pass

    @abstractproperty
    def help(self):
        pass

    @abstractmethod
    def _download(self):
        pass

    @abstractmethod
    def _extract(self):
        pass


    def _rm_compressed(self, **kwargs):
        print("removing compressed files")

        for file in os.listdir(self.path):

            file = os.path.join(self.path, file)

            if not os.path.isdir(file):
                os.remove(file)

    def remove_all_file_with_extension(self, extension):
        for root, dirs, files in os.walk(self.path):
            for file in files:
                file = os.path.join(root, file)
                if file.endswith(".{}".format(extension)):
                    os.remove(file)

    @abstractmethod
    def _process(self):
        pass

    @abstractmethod
    def _rm_raw(self):
        pass

    @abstractmethod
    def reqs(self, **kwargs):
        pass

    @property
    def size(self):
        return self.training_set.size + self.test_set.size


class SubSet(object):
    __metaclass__ = ABCMeta

    def __init__(self, dataset, name):
        self.dataset = dataset
        self._name = name

    def make_dirs(self):
        if not os.path.exists(self.path):
            os.makedirs(self.path)

    @property
    def path(self):
        return os.path.join(self.dataset.path, self._name)

    @abstractmethod
    def dataframe(self):
        pass

    @abstractmethod
    def arrays(self):
        pass

    @abstractmethod
    def random_batch_dataframe_generator(self, batch_size):
        pass

    @abstractmethod
    def random_batch_arrays_generator(self, batch_size):
        pass


    @abstractmethod
    def _load_dataframe(self):
        pass

    @property
    def size(self):
        self._load_dataframe()
        return len(self._dataframe)

    def batch_dataframe_generator(self, batch_size):

        self._load_dataframe()

        i = 0
        total = self.size

        while i < total:

            _from = i
            _to = min(i + batch_size, total)

            idx = list(range(_from, _to))

            yield self._df.iloc[idx]

            i += batch_size
