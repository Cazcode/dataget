{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dataget \u00b6 Dataget is an easy to use, framework-agnostic, dataset library that gives you quick access to a collection of Machine Learning datasets through a simple API. Main features: Minimal : Downloads entire datasets with just 1 line of code. Compatible : Loads data as numpy arrays or pandas dataframes which can be easily used with the majority of Machine Learning frameworks. Transparent : By default stores the data in your current project so you can easily inspect it. Memory Efficient : When a dataset doesn't fit in memory it will return metadata instead so you can iteratively load it. Integrates with Kaggle : Supports loading datasets directly from Kaggle in a variety of formats. Checkout the documentation for the list of available datasets. Getting Started \u00b6 In dataget you just have to do two things: Instantiate a Dataset from our collection. Call the get method to download the data to disk and load it into memory. Both are usually done in one line: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist () . get () This example downloads the MNIST dataset to ./data/image_mnist and loads it as numpy arrays. Kaggle Support \u00b6 Kaggle promotes the use of csv files and dataget loves it! With dataget you can quickly download any dataset from the platform and have immediate access to the data: import dataget df_train , df_test = dataget . kaggle ( \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) To start using Kaggle datasets just make sure you have properly installed and configured the Kaggle API . In the future we want to expand Kaggle support in the following ways: Be able to load any file that numpy or pandas can read. Have generic support for other types of datasets like images, audio, video, etc. e.g dataget.data.kaggle(..., type=\"image\").get(...) Installation \u00b6 dataget is available at pypi so you can use your favorite package manager. pip \u00b6 pip install dataget pipenv \u00b6 pipenv install pytest poetry \u00b6 poetry add dataget Contributing \u00b6 Read our guide on Creating a Dataset if you are interested in adding a dataset to dataget. License \u00b6 MIT License","title":"Introduction"},{"location":"#dataget","text":"Dataget is an easy to use, framework-agnostic, dataset library that gives you quick access to a collection of Machine Learning datasets through a simple API. Main features: Minimal : Downloads entire datasets with just 1 line of code. Compatible : Loads data as numpy arrays or pandas dataframes which can be easily used with the majority of Machine Learning frameworks. Transparent : By default stores the data in your current project so you can easily inspect it. Memory Efficient : When a dataset doesn't fit in memory it will return metadata instead so you can iteratively load it. Integrates with Kaggle : Supports loading datasets directly from Kaggle in a variety of formats. Checkout the documentation for the list of available datasets.","title":"Dataget"},{"location":"#getting-started","text":"In dataget you just have to do two things: Instantiate a Dataset from our collection. Call the get method to download the data to disk and load it into memory. Both are usually done in one line: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist () . get () This example downloads the MNIST dataset to ./data/image_mnist and loads it as numpy arrays.","title":"Getting Started"},{"location":"#kaggle-support","text":"Kaggle promotes the use of csv files and dataget loves it! With dataget you can quickly download any dataset from the platform and have immediate access to the data: import dataget df_train , df_test = dataget . kaggle ( \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) To start using Kaggle datasets just make sure you have properly installed and configured the Kaggle API . In the future we want to expand Kaggle support in the following ways: Be able to load any file that numpy or pandas can read. Have generic support for other types of datasets like images, audio, video, etc. e.g dataget.data.kaggle(..., type=\"image\").get(...)","title":"Kaggle Support"},{"location":"#installation","text":"dataget is available at pypi so you can use your favorite package manager.","title":"Installation"},{"location":"#pip","text":"pip install dataget","title":"pip"},{"location":"#pipenv","text":"pipenv install pytest","title":"pipenv"},{"location":"#poetry","text":"poetry add dataget","title":"poetry"},{"location":"#contributing","text":"Read our guide on Creating a Dataset if you are interested in adding a dataset to dataget.","title":"Contributing"},{"location":"#license","text":"MIT License","title":"License"},{"location":"advanced-usage/","text":"Advanced Usage \u00b6 Specifying Download Directory \u00b6 By default every dataset is downloaded inside a ./data/{dataset_name} folder in the current directory. There are two ways you can control where the data is stored: the first one is use the global_cache keyword argument on the constructor of any Dataset dataget . image . mnist ( global_cache = True ) . get () This will download the dataset inside ~/.dataget/{dataset_name} instead, this is useful if you want to reuse the same dataset across projects. The second is to specify the exact location yourself by using the path keyword argument instead: dataget . image . mnist ( path = \"/my/dataset/path\" ) . get () This gives you full control over the exact location, in this case the dataset will be downloaded /my/dataset/path . \u00b6","title":"Advanced Usage"},{"location":"advanced-usage/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"advanced-usage/#specifying-download-directory","text":"By default every dataset is downloaded inside a ./data/{dataset_name} folder in the current directory. There are two ways you can control where the data is stored: the first one is use the global_cache keyword argument on the constructor of any Dataset dataget . image . mnist ( global_cache = True ) . get () This will download the dataset inside ~/.dataget/{dataset_name} instead, this is useful if you want to reuse the same dataset across projects. The second is to specify the exact location yourself by using the path keyword argument instead: dataget . image . mnist ( path = \"/my/dataset/path\" ) . get () This gives you full control over the exact location, in this case the dataset will be downloaded /my/dataset/path .","title":"Specifying Download Directory"},{"location":"dataset/","text":"Creating a Dataset \u00b6 The Dataset class defined these 4 abstract methods which you must implement: name : a property that should return the name of the dataset e.g. image_mnist . download : the method that should download the dataset to disk and possibly perform other tasks such as file extraction, organization, and clean_cacheup. load : the method that loads the data into memory and possibly structures it in the most convenient format for the user. Path The self.path field is a pathlib.Path that tells the dataset where the data should be stored. The get method ensures this path exists before calling download or load ; use this field when implementing these methods. get kwargs \u00b6 The get method will accept **kwargs which it will forward to load . For example: def load ( self , dtype = np . float32 ): # code With this implementation the get method can be called like this: . get ( dtype = np . uint8 ) Template \u00b6 You can use this template to get started. from dataget.dataset import Dataset class SomeDataset ( Dataset ): # OPTIONAL def __init__ ( self , init_arg , ** kwargs ): # code super () . __init__ ( ** kwargs ) # !!IMPORTANT @property def name ( self ): return \" {dataset_type} _ {dataset_name} \" def download ( self ): # code def load ( self , some_arg ): # code return a , b , c , ... Warning If you are definint you own __init__ remenber to always forward **kwargs to super().__init__ since its important that all datasets support the path and global_cache keyword arguments defined in the Dataset class. If super().__init__ is not called at all the path field will not be instantiated and errors will occure.","title":"Creating a Dataset"},{"location":"dataset/#creating-a-dataset","text":"The Dataset class defined these 4 abstract methods which you must implement: name : a property that should return the name of the dataset e.g. image_mnist . download : the method that should download the dataset to disk and possibly perform other tasks such as file extraction, organization, and clean_cacheup. load : the method that loads the data into memory and possibly structures it in the most convenient format for the user. Path The self.path field is a pathlib.Path that tells the dataset where the data should be stored. The get method ensures this path exists before calling download or load ; use this field when implementing these methods.","title":"Creating a Dataset"},{"location":"dataset/#get-kwargs","text":"The get method will accept **kwargs which it will forward to load . For example: def load ( self , dtype = np . float32 ): # code With this implementation the get method can be called like this: . get ( dtype = np . uint8 )","title":"get kwargs"},{"location":"dataset/#template","text":"You can use this template to get started. from dataget.dataset import Dataset class SomeDataset ( Dataset ): # OPTIONAL def __init__ ( self , init_arg , ** kwargs ): # code super () . __init__ ( ** kwargs ) # !!IMPORTANT @property def name ( self ): return \" {dataset_type} _ {dataset_name} \" def download ( self ): # code def load ( self , some_arg ): # code return a , b , c , ... Warning If you are definint you own __init__ remenber to always forward **kwargs to super().__init__ since its important that all datasets support the path and global_cache keyword arguments defined in the Dataset class. If super().__init__ is not called at all the path field will not be instantiated and errors will occure.","title":"Template"},{"location":"datasets/kaggle/","text":"dataget.kaggle \u00b6 Download any dataset from the Kaggle platform and immediately loads it into memory: import dataget df_train , df_test = dataget . kaggle ( \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) In this example we downloaded the Point Cloud Mnist 2D dataset from Kaggle and load the train.csv and test.csv files as pandas dataframes. Config To start using this Dataset make sure you have properly installed and configured the Kaggle API . Supported Formats \u00b6 Right now we only support the csv format. In the future we want to be able to load any file that numpy or pandas can read. API Reference \u00b6 kaggle \u00b6 __init__ ( self , dataset = None , competition = None , ** kwargs ) \u00b6 Show source code in kaggle.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , dataset : str = None , competition : str = None , ** kwargs ): \"\"\" Create a Kaggle dataset. You have to specify either `dataset` or `competition`. Arguments: dataset: the id of the kaggle dataset in the format `username/dataset_name`. competition: the name of the kaggle competition. kwargs: common init kwargs. \"\"\" assert ( dataset is not None != competition is not None ), \"Set either dataset or competition\" self . kaggle_dataset = dataset self . kaggle_competition = competition super () . __init__ ( ** kwargs ) Create a Kaggle dataset. You have to specify either dataset or competition . Parameters Name Type Description Default dataset str the id of the kaggle dataset in the format username/dataset_name . None competition str the name of the kaggle competition. None **kwargs _empty common init kwargs. {} load ( self , files ) \u00b6 Show source code in kaggle.py 51 52 53 54 55 56 57 def load ( self , files : list ): \"\"\" Arguments: files: the list of files that will be loaded into memory \"\"\" return [ self . _load_file ( filename ) for filename in files ] Parameters Name Type Description Default files list the list of files that will be loaded into memory required","title":"kaggle"},{"location":"datasets/kaggle/#datagetkaggle","text":"Download any dataset from the Kaggle platform and immediately loads it into memory: import dataget df_train , df_test = dataget . kaggle ( \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) In this example we downloaded the Point Cloud Mnist 2D dataset from Kaggle and load the train.csv and test.csv files as pandas dataframes. Config To start using this Dataset make sure you have properly installed and configured the Kaggle API .","title":"dataget.kaggle"},{"location":"datasets/kaggle/#supported-formats","text":"Right now we only support the csv format. In the future we want to be able to load any file that numpy or pandas can read.","title":"Supported Formats"},{"location":"datasets/kaggle/#api-reference","text":"","title":"API Reference"},{"location":"datasets/kaggle/#dataget.kaggle.kaggle","text":"","title":"kaggle"},{"location":"datasets/kaggle/#dataget.kaggle.kaggle.__init__","text":"Show source code in kaggle.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , dataset : str = None , competition : str = None , ** kwargs ): \"\"\" Create a Kaggle dataset. You have to specify either `dataset` or `competition`. Arguments: dataset: the id of the kaggle dataset in the format `username/dataset_name`. competition: the name of the kaggle competition. kwargs: common init kwargs. \"\"\" assert ( dataset is not None != competition is not None ), \"Set either dataset or competition\" self . kaggle_dataset = dataset self . kaggle_competition = competition super () . __init__ ( ** kwargs ) Create a Kaggle dataset. You have to specify either dataset or competition . Parameters Name Type Description Default dataset str the id of the kaggle dataset in the format username/dataset_name . None competition str the name of the kaggle competition. None **kwargs _empty common init kwargs. {}","title":"__init__()"},{"location":"datasets/kaggle/#dataget.kaggle.kaggle.load","text":"Show source code in kaggle.py 51 52 53 54 55 56 57 def load ( self , files : list ): \"\"\" Arguments: files: the list of files that will be loaded into memory \"\"\" return [ self . _load_file ( filename ) for filename in files ] Parameters Name Type Description Default files list the list of files that will be loaded into memory required","title":"load()"},{"location":"datasets/image/cifar10/","text":"dataget.image.cifar10 \u00b6 Downloads the Cifar 10 dataset from University of Toronto's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . cifar10 () . get () Sample \u00b6 Format \u00b6 type shape dtype X_train np.array (50_000, 32, 32, 3) uint8 y_train np.array (50_000, 1) uint8 X_test np.array (10_000, 32, 32, 3) uint8 y_test np.array (10_000, 1) uint8 Info \u00b6 Folder name : image_cifar10 Size on disk : 178M","title":"cifar10"},{"location":"datasets/image/cifar10/#datagetimagecifar10","text":"Downloads the Cifar 10 dataset from University of Toronto's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . cifar10 () . get ()","title":"dataget.image.cifar10"},{"location":"datasets/image/cifar10/#sample","text":"","title":"Sample"},{"location":"datasets/image/cifar10/#format","text":"type shape dtype X_train np.array (50_000, 32, 32, 3) uint8 y_train np.array (50_000, 1) uint8 X_test np.array (10_000, 32, 32, 3) uint8 y_test np.array (10_000, 1) uint8","title":"Format"},{"location":"datasets/image/cifar10/#info","text":"Folder name : image_cifar10 Size on disk : 178M","title":"Info"},{"location":"datasets/image/cifar100/","text":"dataget.image.cifar100 \u00b6 Downloads the Cifar 100 dataset from University of Toronto's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . cifar100 () . get () Sample \u00b6 Format \u00b6 type shape dtype X_train np.array (50_000, 32, 32, 3) uint8 y_train np.array (50_000, 1) uint8 X_test np.array (10_000, 32, 32, 3) uint8 y_test np.array (10_000, 1) uint8 Info \u00b6 Folder name : image_cifar100 Size on disk : 178M","title":"cifar100"},{"location":"datasets/image/cifar100/#datagetimagecifar100","text":"Downloads the Cifar 100 dataset from University of Toronto's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . cifar100 () . get ()","title":"dataget.image.cifar100"},{"location":"datasets/image/cifar100/#sample","text":"","title":"Sample"},{"location":"datasets/image/cifar100/#format","text":"type shape dtype X_train np.array (50_000, 32, 32, 3) uint8 y_train np.array (50_000, 1) uint8 X_test np.array (10_000, 32, 32, 3) uint8 y_test np.array (10_000, 1) uint8","title":"Format"},{"location":"datasets/image/cifar100/#info","text":"Folder name : image_cifar100 Size on disk : 178M","title":"Info"},{"location":"datasets/image/fashion_mnist/","text":"dataget.image.fashion_mnist \u00b6 Downloads the Fashion MNIST dataset and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . fashion_mnist () . get () Sample \u00b6 Format \u00b6 type shape dtype X_train np.array (60_000, 28, 28) uint8 y_train np.array (60_000,) uint8 X_test np.array (10_000, 28, 28) uint8 y_test np.array (10_000,) uint8 Info \u00b6 Folder name : image_fashion_mnist Size on disk : 53MB","title":"fashion_mnist"},{"location":"datasets/image/fashion_mnist/#datagetimagefashion_mnist","text":"Downloads the Fashion MNIST dataset and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . fashion_mnist () . get ()","title":"dataget.image.fashion_mnist"},{"location":"datasets/image/fashion_mnist/#sample","text":"","title":"Sample"},{"location":"datasets/image/fashion_mnist/#format","text":"type shape dtype X_train np.array (60_000, 28, 28) uint8 y_train np.array (60_000,) uint8 X_test np.array (10_000, 28, 28) uint8 y_test np.array (10_000,) uint8","title":"Format"},{"location":"datasets/image/fashion_mnist/#info","text":"Folder name : image_fashion_mnist Size on disk : 53MB","title":"Info"},{"location":"datasets/image/imagenet/","text":"dataget.image.imagenet \u00b6 Downloads the ImageNet dataset from their official ImageNet Object Localization Challenge Kaggle competition and loads its metadata as pandas dataframes. You need the Kaggle CLI installed and configured to use this dataset. import dataget df_train , df_val , df_test = dataget . image . imagenet () . get () Dataget doesn't load the images of this dataset into memory, instead the df_train , df_val , and df_test dataframes has the image_path column which contains the relative path of each sample which you can latter use to iteratively load each image during training. Sample \u00b6 Format \u00b6 type shape df_train pd.DataFrame (544_546, 10) df_val pd.DataFrame (50_000, 9) df_test pd.DataFrame (100_000, 2) Features \u00b6 column type description df_train df_val df_test ImageId str image id x x x image_path str relative path to jpeg image x x x annotations_path str relative path to pascal voc xml x x label str label id x x label_name str label name x x PredictionString str prediction string x x xmin int64 prediction string bouding box coord x x ymin int64 prediction string bouding box coord x x xmax int64 prediction string bouding box coord x x ymax int64 prediction string bouding box coord x x wnid str WordNet ID x Info \u00b6 Folder name : image_imagenet Size on disk : 161GB","title":"imagenet"},{"location":"datasets/image/imagenet/#datagetimageimagenet","text":"Downloads the ImageNet dataset from their official ImageNet Object Localization Challenge Kaggle competition and loads its metadata as pandas dataframes. You need the Kaggle CLI installed and configured to use this dataset. import dataget df_train , df_val , df_test = dataget . image . imagenet () . get () Dataget doesn't load the images of this dataset into memory, instead the df_train , df_val , and df_test dataframes has the image_path column which contains the relative path of each sample which you can latter use to iteratively load each image during training.","title":"dataget.image.imagenet"},{"location":"datasets/image/imagenet/#sample","text":"","title":"Sample"},{"location":"datasets/image/imagenet/#format","text":"type shape df_train pd.DataFrame (544_546, 10) df_val pd.DataFrame (50_000, 9) df_test pd.DataFrame (100_000, 2)","title":"Format"},{"location":"datasets/image/imagenet/#features","text":"column type description df_train df_val df_test ImageId str image id x x x image_path str relative path to jpeg image x x x annotations_path str relative path to pascal voc xml x x label str label id x x label_name str label name x x PredictionString str prediction string x x xmin int64 prediction string bouding box coord x x ymin int64 prediction string bouding box coord x x xmax int64 prediction string bouding box coord x x ymax int64 prediction string bouding box coord x x wnid str WordNet ID x","title":"Features"},{"location":"datasets/image/imagenet/#info","text":"Folder name : image_imagenet Size on disk : 161GB","title":"Info"},{"location":"datasets/image/mnist/","text":"dataget.image.mnist \u00b6 Downloads the MNIST dataset from Yann LeCun's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . mnist () . get () Sample \u00b6 Format \u00b6 type shape dtype X_train np.array (60_000, 28, 28) uint8 y_train np.array (60_000,) uint8 X_test np.array (10_000, 28, 28) uint8 y_test np.array (10_000,) uint8 Info \u00b6 Folder name : image_mnist Size on disk : 53MB","title":"mnist"},{"location":"datasets/image/mnist/#datagetimagemnist","text":"Downloads the MNIST dataset from Yann LeCun's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . mnist () . get ()","title":"dataget.image.mnist"},{"location":"datasets/image/mnist/#sample","text":"","title":"Sample"},{"location":"datasets/image/mnist/#format","text":"type shape dtype X_train np.array (60_000, 28, 28) uint8 y_train np.array (60_000,) uint8 X_test np.array (10_000, 28, 28) uint8 y_test np.array (10_000,) uint8","title":"Format"},{"location":"datasets/image/mnist/#info","text":"Folder name : image_mnist Size on disk : 53MB","title":"Info"},{"location":"datasets/structured/movielens/movielens_20m/","text":"dataget.structured.movielens_20m \u00b6 Downloads the MovieLens 20M dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , genome_scores , genome_tags , ) = dataget . structured . movielens_20m () . get () Format \u00b6 type shape ratings pd.DataFrame (20_000_263, 4) movies pd.DataFrame (27_278, 3) tags pd.DataFrame (465_564, 4) links pd.DataFrame (27_278, 3) genome_scores pd.DataFrame (11_709_768, 3) genome_tags pd.DataFrame (1_128, 2) Features \u00b6 ratings \u00b6 column type userId int64 movieId int64 rating float64 timestamp int64 movies \u00b6 column type movieId int64 title object genres object tags \u00b6 column type movieId int64 imdbId int64 tmdbId float64 genome_scores \u00b6 column type movieId int64 tagId int64 relevance float64 genome_tags \u00b6 column type tagId int64 tag object Info \u00b6 Folder name : structured_movielens_20m Size on disk : 836MB","title":"movielens_20m"},{"location":"datasets/structured/movielens/movielens_20m/#datagetstructuredmovielens_20m","text":"Downloads the MovieLens 20M dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , genome_scores , genome_tags , ) = dataget . structured . movielens_20m () . get ()","title":"dataget.structured.movielens_20m"},{"location":"datasets/structured/movielens/movielens_20m/#format","text":"type shape ratings pd.DataFrame (20_000_263, 4) movies pd.DataFrame (27_278, 3) tags pd.DataFrame (465_564, 4) links pd.DataFrame (27_278, 3) genome_scores pd.DataFrame (11_709_768, 3) genome_tags pd.DataFrame (1_128, 2)","title":"Format"},{"location":"datasets/structured/movielens/movielens_20m/#features","text":"","title":"Features"},{"location":"datasets/structured/movielens/movielens_20m/#ratings","text":"column type userId int64 movieId int64 rating float64 timestamp int64","title":"ratings"},{"location":"datasets/structured/movielens/movielens_20m/#movies","text":"column type movieId int64 title object genres object","title":"movies"},{"location":"datasets/structured/movielens/movielens_20m/#tags","text":"column type movieId int64 imdbId int64 tmdbId float64","title":"tags"},{"location":"datasets/structured/movielens/movielens_20m/#genome_scores","text":"column type movieId int64 tagId int64 relevance float64","title":"genome_scores"},{"location":"datasets/structured/movielens/movielens_20m/#genome_tags","text":"column type tagId int64 tag object","title":"genome_tags"},{"location":"datasets/structured/movielens/movielens_20m/#info","text":"Folder name : structured_movielens_20m Size on disk : 836MB","title":"Info"},{"location":"datasets/structured/movielens/movielens_25m/","text":"dataget.structured.movielens_25m \u00b6 Downloads the MovieLens 25M dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , genome_scores , genome_tags , ) = dataget . structured . movielens_25m () . get () Format \u00b6 type shape ratings pd.DataFrame (25_000_095, 4) movies pd.DataFrame (62_423, 3) tags pd.DataFrame (1_093_360, 4) links pd.DataFrame (62_423, 3) genome_scores pd.DataFrame (15_584_448, 3) genome_tags pd.DataFrame (1_128, 2) Features \u00b6 ratings \u00b6 column type userId int64 movieId int64 rating float64 timestamp int64 movies \u00b6 column type movieId int64 title object genres object tags \u00b6 column type movieId int64 imdbId int64 tmdbId float64 genome_scores \u00b6 column type movieId int64 tagId int64 relevance float64 genome_tags \u00b6 column type tagId int64 tag object Info \u00b6 Folder name : structured_movielens_25m Size on disk : 1.1GB","title":"movielens_25m"},{"location":"datasets/structured/movielens/movielens_25m/#datagetstructuredmovielens_25m","text":"Downloads the MovieLens 25M dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , genome_scores , genome_tags , ) = dataget . structured . movielens_25m () . get ()","title":"dataget.structured.movielens_25m"},{"location":"datasets/structured/movielens/movielens_25m/#format","text":"type shape ratings pd.DataFrame (25_000_095, 4) movies pd.DataFrame (62_423, 3) tags pd.DataFrame (1_093_360, 4) links pd.DataFrame (62_423, 3) genome_scores pd.DataFrame (15_584_448, 3) genome_tags pd.DataFrame (1_128, 2)","title":"Format"},{"location":"datasets/structured/movielens/movielens_25m/#features","text":"","title":"Features"},{"location":"datasets/structured/movielens/movielens_25m/#ratings","text":"column type userId int64 movieId int64 rating float64 timestamp int64","title":"ratings"},{"location":"datasets/structured/movielens/movielens_25m/#movies","text":"column type movieId int64 title object genres object","title":"movies"},{"location":"datasets/structured/movielens/movielens_25m/#tags","text":"column type movieId int64 imdbId int64 tmdbId float64","title":"tags"},{"location":"datasets/structured/movielens/movielens_25m/#genome_scores","text":"column type movieId int64 tagId int64 relevance float64","title":"genome_scores"},{"location":"datasets/structured/movielens/movielens_25m/#genome_tags","text":"column type tagId int64 tag object","title":"genome_tags"},{"location":"datasets/structured/movielens/movielens_25m/#info","text":"Folder name : structured_movielens_25m Size on disk : 1.1GB","title":"Info"},{"location":"datasets/structured/movielens/movielens_latest/","text":"dataget.structured.movielens_latest \u00b6 Downloads the MovieLens Latest dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , genome_scores , genome_tags , ) = dataget . structured . movielens_latest () . get () Warning This dataset is updates automatically. If you need consistency use movielens_25m or movielens_20m instead. Format \u00b6 type shape ratings pd.DataFrame (27_753_444, 4) movies pd.DataFrame (58_098, 3) tags pd.DataFrame (1_108_997, 4) links pd.DataFrame (58_098, 3) genome_scores pd.DataFrame (14_862_528, 3) genome_tags pd.DataFrame (1_128, 2) Features \u00b6 ratings \u00b6 column type userId int64 movieId int64 rating float64 timestamp int64 movies \u00b6 column type movieId int64 title object genres object tags \u00b6 column type movieId int64 imdbId int64 tmdbId float64 genome_scores \u00b6 column type movieId int64 tagId int64 relevance float64 genome_tags \u00b6 column type tagId int64 tag object Info \u00b6 Folder name : structured_movielens_latest Size on disk : 1.2GB","title":"movielens_latest"},{"location":"datasets/structured/movielens/movielens_latest/#datagetstructuredmovielens_latest","text":"Downloads the MovieLens Latest dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , genome_scores , genome_tags , ) = dataget . structured . movielens_latest () . get () Warning This dataset is updates automatically. If you need consistency use movielens_25m or movielens_20m instead.","title":"dataget.structured.movielens_latest"},{"location":"datasets/structured/movielens/movielens_latest/#format","text":"type shape ratings pd.DataFrame (27_753_444, 4) movies pd.DataFrame (58_098, 3) tags pd.DataFrame (1_108_997, 4) links pd.DataFrame (58_098, 3) genome_scores pd.DataFrame (14_862_528, 3) genome_tags pd.DataFrame (1_128, 2)","title":"Format"},{"location":"datasets/structured/movielens/movielens_latest/#features","text":"","title":"Features"},{"location":"datasets/structured/movielens/movielens_latest/#ratings","text":"column type userId int64 movieId int64 rating float64 timestamp int64","title":"ratings"},{"location":"datasets/structured/movielens/movielens_latest/#movies","text":"column type movieId int64 title object genres object","title":"movies"},{"location":"datasets/structured/movielens/movielens_latest/#tags","text":"column type movieId int64 imdbId int64 tmdbId float64","title":"tags"},{"location":"datasets/structured/movielens/movielens_latest/#genome_scores","text":"column type movieId int64 tagId int64 relevance float64","title":"genome_scores"},{"location":"datasets/structured/movielens/movielens_latest/#genome_tags","text":"column type tagId int64 tag object","title":"genome_tags"},{"location":"datasets/structured/movielens/movielens_latest/#info","text":"Folder name : structured_movielens_latest Size on disk : 1.2GB","title":"Info"},{"location":"datasets/structured/movielens/movielens_latest_small/","text":"dataget.structured.movielens_latest_small \u00b6 Downloads the MovieLens Latest Small dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , ) = dataget . structured . movielens_latest_small () . get () Warning This dataset is updates automatically. If you need consistency use movielens_25m or movielens_20m instead. Format \u00b6 type shape ratings pd.DataFrame (100_836, 4) movies pd.DataFrame (9_742, 3) tags pd.DataFrame (3_683, 4) links pd.DataFrame (9_742, 3) Features \u00b6 ratings \u00b6 column type userId int64 movieId int64 rating float64 timestamp int64 movies \u00b6 column type movieId int64 title object genres object tags \u00b6 column type movieId int64 imdbId int64 tmdbId float64 genome_scores \u00b6 column type movieId int64 tagId int64 relevance float64 genome_tags \u00b6 column type tagId int64 tag object Info \u00b6 Folder name : structured_movielens_latest_small Size on disk : 3.2MB","title":"movielens_latest_small"},{"location":"datasets/structured/movielens/movielens_latest_small/#datagetstructuredmovielens_latest_small","text":"Downloads the MovieLens Latest Small dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , ) = dataget . structured . movielens_latest_small () . get () Warning This dataset is updates automatically. If you need consistency use movielens_25m or movielens_20m instead.","title":"dataget.structured.movielens_latest_small"},{"location":"datasets/structured/movielens/movielens_latest_small/#format","text":"type shape ratings pd.DataFrame (100_836, 4) movies pd.DataFrame (9_742, 3) tags pd.DataFrame (3_683, 4) links pd.DataFrame (9_742, 3)","title":"Format"},{"location":"datasets/structured/movielens/movielens_latest_small/#features","text":"","title":"Features"},{"location":"datasets/structured/movielens/movielens_latest_small/#ratings","text":"column type userId int64 movieId int64 rating float64 timestamp int64","title":"ratings"},{"location":"datasets/structured/movielens/movielens_latest_small/#movies","text":"column type movieId int64 title object genres object","title":"movies"},{"location":"datasets/structured/movielens/movielens_latest_small/#tags","text":"column type movieId int64 imdbId int64 tmdbId float64","title":"tags"},{"location":"datasets/structured/movielens/movielens_latest_small/#genome_scores","text":"column type movieId int64 tagId int64 relevance float64","title":"genome_scores"},{"location":"datasets/structured/movielens/movielens_latest_small/#genome_tags","text":"column type tagId int64 tag object","title":"genome_tags"},{"location":"datasets/structured/movielens/movielens_latest_small/#info","text":"Folder name : structured_movielens_latest_small Size on disk : 3.2MB","title":"Info"},{"location":"datasets/structured/movielens/movielens_synthetic_1b/","text":"dataget.structured.movielens_synthetic_1b \u00b6 Downloads the MovieLens Synthetic 1B dataset returns an iterable of numpy numpy arrays. import dataget ratings_iterable = dataget . structured . movielens_synthetic_1b () . get () By default each array has a different length (the dataset is organized like this), to make training easier we provide the batch_size keyword argument which you can use to get arrays of a given length. import dataget ratings_iterable = dataget . structured . movielens_synthetic_1b () . get ( batch_size = 64 ) Note Depending of the batch_size the last array may have a smaller size than the rest. If you have enough memory you can consider concatenating all into a single array: import dataget ratings_iterable = dataget . structured . movielens_synthetic_1b () . get () ratings = np . concatenate ( ratings_iterable , axis = 0 ) Format \u00b6 type full shape dtype ratings_iterable Iterable[np.array] (1_226_159_268, 2) int64 Features \u00b6 column description total 0 users 22_10_078 1 movies 855_776 Info \u00b6 Folder name : structured_movielens_synthetic_1b Size on disk : 3.1GB","title":"movielens_synthetic_1b"},{"location":"datasets/structured/movielens/movielens_synthetic_1b/#datagetstructuredmovielens_synthetic_1b","text":"Downloads the MovieLens Synthetic 1B dataset returns an iterable of numpy numpy arrays. import dataget ratings_iterable = dataget . structured . movielens_synthetic_1b () . get () By default each array has a different length (the dataset is organized like this), to make training easier we provide the batch_size keyword argument which you can use to get arrays of a given length. import dataget ratings_iterable = dataget . structured . movielens_synthetic_1b () . get ( batch_size = 64 ) Note Depending of the batch_size the last array may have a smaller size than the rest. If you have enough memory you can consider concatenating all into a single array: import dataget ratings_iterable = dataget . structured . movielens_synthetic_1b () . get () ratings = np . concatenate ( ratings_iterable , axis = 0 )","title":"dataget.structured.movielens_synthetic_1b"},{"location":"datasets/structured/movielens/movielens_synthetic_1b/#format","text":"type full shape dtype ratings_iterable Iterable[np.array] (1_226_159_268, 2) int64","title":"Format"},{"location":"datasets/structured/movielens/movielens_synthetic_1b/#features","text":"column description total 0 users 22_10_078 1 movies 855_776","title":"Features"},{"location":"datasets/structured/movielens/movielens_synthetic_1b/#info","text":"Folder name : structured_movielens_synthetic_1b Size on disk : 3.1GB","title":"Info"},{"location":"datasets/text/imdb_reviews/","text":"dataget.text.imdb_reviews \u00b6 Downloads the IMDB Reviews dataset and loads it as pandas dataframes. import dataget df_train , df_test = dataget . text . imdb_reviews () . get () This dataset also contains unsupervised sample, to load them set the include_unsupervised argument: import dataget df_train , df_test = dataget . text . imdb_reviews () . get ( include_unsupervised = True ) All unsupervised sample will have a label of -1 . Format \u00b6 type shape df_train pd.DataFrame (75_000, 3) df_test pd.DataFrame (25_000, 3) Features \u00b6 column type text str label int64 text_path str Info \u00b6 Folder name : text_imdb_reviews Size on disk : 490MB API Reference \u00b6 imdb_reviews \u00b6 load ( self , include_unlabeled = False ) \u00b6 Show source code in imdb_reviews.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def load ( self , include_unlabeled = False ): \"\"\" Arguments: include_unlabeled: whether or not to include the unlabeled samples. \"\"\" train_path = self . path / \"aclImdb\" / \"train\" test_path = self . path / \"aclImdb\" / \"test\" # train df_train = [ self . load_df ( train_path / \"pos\" , label = 1 ), self . load_df ( train_path / \"neg\" , label = 0 ), ] if include_unlabeled : df_train . append ( self . load_df ( train_path / \"unsup\" , label =- 1 )) df_train = pd . concat ( df_train , axis = 0 ) # test df_test = pd . concat ( [ self . load_df ( test_path / \"pos\" , label = 1 ), self . load_df ( test_path / \"neg\" , label = 0 ), ], axis = 0 , ) return df_train , df_test Parameters Name Type Description Default include_unlabeled _empty whether or not to include the unlabeled samples. False","title":"imdb_reviews"},{"location":"datasets/text/imdb_reviews/#datagettextimdb_reviews","text":"Downloads the IMDB Reviews dataset and loads it as pandas dataframes. import dataget df_train , df_test = dataget . text . imdb_reviews () . get () This dataset also contains unsupervised sample, to load them set the include_unsupervised argument: import dataget df_train , df_test = dataget . text . imdb_reviews () . get ( include_unsupervised = True ) All unsupervised sample will have a label of -1 .","title":"dataget.text.imdb_reviews"},{"location":"datasets/text/imdb_reviews/#format","text":"type shape df_train pd.DataFrame (75_000, 3) df_test pd.DataFrame (25_000, 3)","title":"Format"},{"location":"datasets/text/imdb_reviews/#features","text":"column type text str label int64 text_path str","title":"Features"},{"location":"datasets/text/imdb_reviews/#info","text":"Folder name : text_imdb_reviews Size on disk : 490MB","title":"Info"},{"location":"datasets/text/imdb_reviews/#api-reference","text":"","title":"API Reference"},{"location":"datasets/text/imdb_reviews/#dataget.text.imdb_reviews.imdb_reviews","text":"","title":"imdb_reviews"},{"location":"datasets/text/imdb_reviews/#dataget.text.imdb_reviews.imdb_reviews.load","text":"Show source code in imdb_reviews.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def load ( self , include_unlabeled = False ): \"\"\" Arguments: include_unlabeled: whether or not to include the unlabeled samples. \"\"\" train_path = self . path / \"aclImdb\" / \"train\" test_path = self . path / \"aclImdb\" / \"test\" # train df_train = [ self . load_df ( train_path / \"pos\" , label = 1 ), self . load_df ( train_path / \"neg\" , label = 0 ), ] if include_unlabeled : df_train . append ( self . load_df ( train_path / \"unsup\" , label =- 1 )) df_train = pd . concat ( df_train , axis = 0 ) # test df_test = pd . concat ( [ self . load_df ( test_path / \"pos\" , label = 1 ), self . load_df ( test_path / \"neg\" , label = 0 ), ], axis = 0 , ) return df_train , df_test Parameters Name Type Description Default include_unlabeled _empty whether or not to include the unlabeled samples. False","title":"load()"},{"location":"datasets/toy/spirals/","text":"dataget.toy.spirals \u00b6 This is an artificial dataset created using polar functions inspired by a similar dataset found in tensorflow playground . import dataget df_train , df_test = dataget . toy . spirals () . get () Sample \u00b6 Format \u00b6 type shape df_train pd.DataFrame (399, 3) df_test pd.DataFrame (45, 3) Features \u00b6 column type description x0 float coordinate x1 float coordinate y int label Info \u00b6 Folder name : toy_spirals Size on disk : 24KB","title":"spirals"},{"location":"datasets/toy/spirals/#datagettoyspirals","text":"This is an artificial dataset created using polar functions inspired by a similar dataset found in tensorflow playground . import dataget df_train , df_test = dataget . toy . spirals () . get ()","title":"dataget.toy.spirals"},{"location":"datasets/toy/spirals/#sample","text":"","title":"Sample"},{"location":"datasets/toy/spirals/#format","text":"type shape df_train pd.DataFrame (399, 3) df_test pd.DataFrame (45, 3)","title":"Format"},{"location":"datasets/toy/spirals/#features","text":"column type description x0 float coordinate x1 float coordinate y int label","title":"Features"},{"location":"datasets/toy/spirals/#info","text":"Folder name : toy_spirals Size on disk : 24KB","title":"Info"}]}