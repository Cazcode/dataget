{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dataget \u00b6 Dataget is an easy to use, framework-agnostic, dataset library that gives you quick access to a collection of Machine Learning datasets through a simple API. Main features: Minimal : Downloads entire datasets with just 1 line of code. Compatible : Loads data as numpy arrays or pandas dataframes which can be easily used with the majority of Machine Learning frameworks. Transparent : By default stores the data in your current project so you can easily inspect it. Memory Efficient : When a dataset doesn't fit in memory it will return metadata instead so you can iteratively load it. Integrates with Kaggle : Supports loading datasets directly from Kaggle in a variety of formats. Checkout the documentation for the list of avaiable datasets. Getting Started \u00b6 In dataget you just have to do two things: Instantiate a Dataset from our collection. Call the get method to download the data to disk and load it into memory. Both are usually done in one line: import dataget X_train , y_train , X_test , y_test = dataget . vision . mnist () . get () This examples downloads the MNIST dataset to ./data/vision_mnist and loads it as numpy arrays. Kaggle Support \u00b6 Kaggle promotes the use of csv files and dataget loves it! With dataget you can quickly download any dataset from the platform and have immediate access to the data: import dataget df_train , df_test = dataget . kaggle ( \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) To start using Kaggle datasets just make sure you have properly installed and configured the Kaggle API . In the future we want to expand Kaggle support in the following ways: Be able to load any file that numpy or pandas can read. Have generic support for other types of datasets like images, audio, video, etc. e.g dataget.data.kaggle(..., type=\"vision\").get(...) Installation \u00b6 dataget is avaiable at pypi so you can use your favorite package manager. pip \u00b6 pip install dataget pipenv \u00b6 pipenv install pytest poetry \u00b6 poetry add dataget Contributing \u00b6 Read our guide one Creating a Dataset if you are interested in adding a dataset to dataget. License \u00b6 MIT License","title":"Introduction"},{"location":"#dataget","text":"Dataget is an easy to use, framework-agnostic, dataset library that gives you quick access to a collection of Machine Learning datasets through a simple API. Main features: Minimal : Downloads entire datasets with just 1 line of code. Compatible : Loads data as numpy arrays or pandas dataframes which can be easily used with the majority of Machine Learning frameworks. Transparent : By default stores the data in your current project so you can easily inspect it. Memory Efficient : When a dataset doesn't fit in memory it will return metadata instead so you can iteratively load it. Integrates with Kaggle : Supports loading datasets directly from Kaggle in a variety of formats. Checkout the documentation for the list of avaiable datasets.","title":"Dataget"},{"location":"#getting-started","text":"In dataget you just have to do two things: Instantiate a Dataset from our collection. Call the get method to download the data to disk and load it into memory. Both are usually done in one line: import dataget X_train , y_train , X_test , y_test = dataget . vision . mnist () . get () This examples downloads the MNIST dataset to ./data/vision_mnist and loads it as numpy arrays.","title":"Getting Started"},{"location":"#kaggle-support","text":"Kaggle promotes the use of csv files and dataget loves it! With dataget you can quickly download any dataset from the platform and have immediate access to the data: import dataget df_train , df_test = dataget . kaggle ( \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) To start using Kaggle datasets just make sure you have properly installed and configured the Kaggle API . In the future we want to expand Kaggle support in the following ways: Be able to load any file that numpy or pandas can read. Have generic support for other types of datasets like images, audio, video, etc. e.g dataget.data.kaggle(..., type=\"vision\").get(...)","title":"Kaggle Support"},{"location":"#installation","text":"dataget is avaiable at pypi so you can use your favorite package manager.","title":"Installation"},{"location":"#pip","text":"pip install dataget","title":"pip"},{"location":"#pipenv","text":"pipenv install pytest","title":"pipenv"},{"location":"#poetry","text":"poetry add dataget","title":"poetry"},{"location":"#contributing","text":"Read our guide one Creating a Dataset if you are interested in adding a dataset to dataget.","title":"Contributing"},{"location":"#license","text":"MIT License","title":"License"},{"location":"advanced-usage/","text":"Advanced Usage \u00b6 Dataset Directory \u00b6 By default every dataset is downloaded inside a ./data/{dataset_name} folder in the current directory. There are two ways you can control where the data is stored: the first one is use the global_cache keyword argument on the constructor of any Dataset dataget . vision . mnist ( global_cache = True ) . get () This will download the dataset inside ~/.dataget/{dataset_name} instead, this is useful if you want to reuse the same dataset across projects. The second is to specify the exact location yourself by using the path keyword argument instead: dataget . vision . mnist ( path = \"/my/dataset/path\" ) . get () This gives you full control over the exact location, in this case the dataset will be downloaded /my/dataset/path . \u00b6","title":"Advanced Usage"},{"location":"advanced-usage/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"advanced-usage/#dataset-directory","text":"By default every dataset is downloaded inside a ./data/{dataset_name} folder in the current directory. There are two ways you can control where the data is stored: the first one is use the global_cache keyword argument on the constructor of any Dataset dataget . vision . mnist ( global_cache = True ) . get () This will download the dataset inside ~/.dataget/{dataset_name} instead, this is useful if you want to reuse the same dataset across projects. The second is to specify the exact location yourself by using the path keyword argument instead: dataget . vision . mnist ( path = \"/my/dataset/path\" ) . get () This gives you full control over the exact location, in this case the dataset will be downloaded /my/dataset/path .","title":"Dataset Directory"},{"location":"dataset/","text":"Creating a Dataset \u00b6 The Dataset class defined these 4 abstract methods which you must implement: name : a property that should return the name of the dataset e.g. vision_mnist . download : the method that should download the dataset to disk and possibly perform other tasks such as file extraction, organization, and clean_cacheup. load : the method that loads the data into memory and possibly structures it in the most convenient format for the user. is_valid : a method which validates that the data on disk is valid to guarantee that the download process succeded. Path The self.path field is a pathlib.Path that tells the dataset where the data should be stored. The get method ensures this path exists before calling is_valid , download , or load ; use this field when implementing these methods. get kwargs \u00b6 The get method will accept **kwargs which it will forward to download , load , and is_valid so all these methods have to accept the same arguments. An alternatively strategy is have each accept its desired required or optional arguments and accept **kwargs to accumulate the additional it doesn't need. For example: def download ( self , version , limit = None , ** kwargs ): # code def load ( self , dtype = np . float32 , ** kwargs ): # code With this implementation you the get method could be called like this: . get ( version = \"0.0.2\" , dtype = np . uint8 ) Note In the previous example the version argument is required because download uses it as a positional argument although for get its just a keyword argument, if it were not passed a TypeError would've been raised. Template \u00b6 You can use this template to get started. from dataget.dataset import Dataset class SomeDataset ( Dataset ): # OPTIONAL def __init__ ( self , init_arg , ** kwargs ): # code super () . __init__ ( ** kwargs ) # !!IMPORTANT @property def name ( self ): return \"some_dataset_name\" def download ( self , some_arg , ** kwargs ): # code def load ( self , other_arg , ** kwargs ): # code return data1 , data2 , data3 , ... def is_valid ( self , another_arg , ** kwargs ): # code return True | False Warning If you are definint you own __init__ remenber to always forward **kwargs to super().__init__ since its important that all datasets support the path and global_cache keyword arguments defined in the Dataset class. If super().__init__ is not called at all the path field will not be instantiated and errors will occure.","title":"Creating a Dataset"},{"location":"dataset/#creating-a-dataset","text":"The Dataset class defined these 4 abstract methods which you must implement: name : a property that should return the name of the dataset e.g. vision_mnist . download : the method that should download the dataset to disk and possibly perform other tasks such as file extraction, organization, and clean_cacheup. load : the method that loads the data into memory and possibly structures it in the most convenient format for the user. is_valid : a method which validates that the data on disk is valid to guarantee that the download process succeded. Path The self.path field is a pathlib.Path that tells the dataset where the data should be stored. The get method ensures this path exists before calling is_valid , download , or load ; use this field when implementing these methods.","title":"Creating a Dataset"},{"location":"dataset/#get-kwargs","text":"The get method will accept **kwargs which it will forward to download , load , and is_valid so all these methods have to accept the same arguments. An alternatively strategy is have each accept its desired required or optional arguments and accept **kwargs to accumulate the additional it doesn't need. For example: def download ( self , version , limit = None , ** kwargs ): # code def load ( self , dtype = np . float32 , ** kwargs ): # code With this implementation you the get method could be called like this: . get ( version = \"0.0.2\" , dtype = np . uint8 ) Note In the previous example the version argument is required because download uses it as a positional argument although for get its just a keyword argument, if it were not passed a TypeError would've been raised.","title":"get kwargs"},{"location":"dataset/#template","text":"You can use this template to get started. from dataget.dataset import Dataset class SomeDataset ( Dataset ): # OPTIONAL def __init__ ( self , init_arg , ** kwargs ): # code super () . __init__ ( ** kwargs ) # !!IMPORTANT @property def name ( self ): return \"some_dataset_name\" def download ( self , some_arg , ** kwargs ): # code def load ( self , other_arg , ** kwargs ): # code return data1 , data2 , data3 , ... def is_valid ( self , another_arg , ** kwargs ): # code return True | False Warning If you are definint you own __init__ remenber to always forward **kwargs to super().__init__ since its important that all datasets support the path and global_cache keyword arguments defined in the Dataset class. If super().__init__ is not called at all the path field will not be instantiated and errors will occure.","title":"Template"},{"location":"datasets/kaggle/","text":"dataget.kaggle \u00b6 Download any dataset from the Kaggle platform and immediately loads it into memory: import dataget df_train , df_test = dataget . kaggle ( \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) In this example we downloaded the Point Cloud Mnist 2D dataset from Kaggle and load the train.csv and test.csv files as pandas dataframes. Config To start using this Dataset make sure you have properly installed and configured the Kaggle API . Supported Formats \u00b6 Right now we only support the csv format. In the future we want to be able to load any file that numpy or pandas can read. Reference API \u00b6 kaggle \u00b6 __init__ ( self , dataset , ** kwargs ) \u00b6 Show source code in kaggle.py 15 16 17 18 19 20 21 22 def __init__ ( self , dataset : str , ** kwargs ): \"\"\" Arguments: dataset: the id of the kaggle dataset in the format `username/dataset_name` \"\"\" self . kaggle_dataset = dataset super () . __init__ ( ** kwargs ) Parameters Name Type Description Default dataset str the id of the kaggle dataset in the format username/dataset_name required load ( self , files , ** kwargs ) \u00b6 Show source code in kaggle.py 30 31 32 33 34 35 def load ( self , files : list , ** kwargs ): \"\"\" Arguments: files: the list of files that will be loaded into memory \"\"\" return [ self . _load_file ( filename ) for filename in files ] Parameters Name Type Description Default files list the list of files that will be loaded into memory required","title":"kaggle"},{"location":"datasets/kaggle/#datagetkaggle","text":"Download any dataset from the Kaggle platform and immediately loads it into memory: import dataget df_train , df_test = dataget . kaggle ( \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) In this example we downloaded the Point Cloud Mnist 2D dataset from Kaggle and load the train.csv and test.csv files as pandas dataframes. Config To start using this Dataset make sure you have properly installed and configured the Kaggle API .","title":"dataget.kaggle"},{"location":"datasets/kaggle/#supported-formats","text":"Right now we only support the csv format. In the future we want to be able to load any file that numpy or pandas can read.","title":"Supported Formats"},{"location":"datasets/kaggle/#reference-api","text":"","title":"Reference API"},{"location":"datasets/kaggle/#dataget.kaggle.kaggle","text":"","title":"kaggle"},{"location":"datasets/kaggle/#dataget.kaggle.kaggle.__init__","text":"Show source code in kaggle.py 15 16 17 18 19 20 21 22 def __init__ ( self , dataset : str , ** kwargs ): \"\"\" Arguments: dataset: the id of the kaggle dataset in the format `username/dataset_name` \"\"\" self . kaggle_dataset = dataset super () . __init__ ( ** kwargs ) Parameters Name Type Description Default dataset str the id of the kaggle dataset in the format username/dataset_name required","title":"__init__()"},{"location":"datasets/kaggle/#dataget.kaggle.kaggle.load","text":"Show source code in kaggle.py 30 31 32 33 34 35 def load ( self , files : list , ** kwargs ): \"\"\" Arguments: files: the list of files that will be loaded into memory \"\"\" return [ self . _load_file ( filename ) for filename in files ] Parameters Name Type Description Default files list the list of files that will be loaded into memory required","title":"load()"},{"location":"datasets/vision/mnist/","text":"dataget.vision.mnist \u00b6 Downloads the MNIST dataset from Yann LeCun's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . vision . mnist () . get () Format \u00b6 type shape dtype X_train np.array (60000, 28, 28) uint8 y_train np.array (60000,) uint8 X_test np.array (10000, 28, 28) uint8 y_test np.array (10000,) uint8 Info \u00b6 Folder name : vision_mnist Size on disk : 53MB","title":"mnist"},{"location":"datasets/vision/mnist/#datagetvisionmnist","text":"Downloads the MNIST dataset from Yann LeCun's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . vision . mnist () . get ()","title":"dataget.vision.mnist"},{"location":"datasets/vision/mnist/#format","text":"type shape dtype X_train np.array (60000, 28, 28) uint8 y_train np.array (60000,) uint8 X_test np.array (10000, 28, 28) uint8 y_test np.array (10000,) uint8","title":"Format"},{"location":"datasets/vision/mnist/#info","text":"Folder name : vision_mnist Size on disk : 53MB","title":"Info"}]}