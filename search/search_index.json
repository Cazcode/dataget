{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dataget \u00b6 Dataget is an easy to use, framework-agnostic, dataset library that gives you quick access to a collection of Machine Learning datasets through a simple API. Main features: Minimal : Downloads entire datasets with just 1 line of code. Compatible : Loads data as numpy arrays or pandas dataframes which can be easily used with the majority of Machine Learning frameworks. Transparent : By default stores the data in your current project so you can easily inspect it. Memory Efficient : When a dataset doesn't fit in memory it will return metadata instead so you can iteratively load it. Integrates with Kaggle : Supports loading datasets directly from Kaggle in a variety of formats. Checkout the documentation for the list of available datasets. Getting Started \u00b6 In dataget you just have to do two things: Instantiate a Dataset from our collection. Call the get method to download the data to disk and load it into memory. Both are usually done in one line: import dataget X_train , y_train , X_test , y_test = dataget . vision . mnist () . get () This example downloads the MNIST dataset to ./data/vision_mnist and loads it as numpy arrays. Kaggle Support \u00b6 Kaggle promotes the use of csv files and dataget loves it! With dataget you can quickly download any dataset from the platform and have immediate access to the data: import dataget df_train , df_test = dataget . kaggle ( \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) To start using Kaggle datasets just make sure you have properly installed and configured the Kaggle API . In the future we want to expand Kaggle support in the following ways: Be able to load any file that numpy or pandas can read. Have generic support for other types of datasets like images, audio, video, etc. e.g dataget.data.kaggle(..., type=\"vision\").get(...) Installation \u00b6 dataget is available at pypi so you can use your favorite package manager. pip \u00b6 pip install dataget pipenv \u00b6 pipenv install pytest poetry \u00b6 poetry add dataget Contributing \u00b6 Read our guide on Creating a Dataset if you are interested in adding a dataset to dataget. License \u00b6 MIT License","title":"Introduction"},{"location":"#dataget","text":"Dataget is an easy to use, framework-agnostic, dataset library that gives you quick access to a collection of Machine Learning datasets through a simple API. Main features: Minimal : Downloads entire datasets with just 1 line of code. Compatible : Loads data as numpy arrays or pandas dataframes which can be easily used with the majority of Machine Learning frameworks. Transparent : By default stores the data in your current project so you can easily inspect it. Memory Efficient : When a dataset doesn't fit in memory it will return metadata instead so you can iteratively load it. Integrates with Kaggle : Supports loading datasets directly from Kaggle in a variety of formats. Checkout the documentation for the list of available datasets.","title":"Dataget"},{"location":"#getting-started","text":"In dataget you just have to do two things: Instantiate a Dataset from our collection. Call the get method to download the data to disk and load it into memory. Both are usually done in one line: import dataget X_train , y_train , X_test , y_test = dataget . vision . mnist () . get () This example downloads the MNIST dataset to ./data/vision_mnist and loads it as numpy arrays.","title":"Getting Started"},{"location":"#kaggle-support","text":"Kaggle promotes the use of csv files and dataget loves it! With dataget you can quickly download any dataset from the platform and have immediate access to the data: import dataget df_train , df_test = dataget . kaggle ( \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) To start using Kaggle datasets just make sure you have properly installed and configured the Kaggle API . In the future we want to expand Kaggle support in the following ways: Be able to load any file that numpy or pandas can read. Have generic support for other types of datasets like images, audio, video, etc. e.g dataget.data.kaggle(..., type=\"vision\").get(...)","title":"Kaggle Support"},{"location":"#installation","text":"dataget is available at pypi so you can use your favorite package manager.","title":"Installation"},{"location":"#pip","text":"pip install dataget","title":"pip"},{"location":"#pipenv","text":"pipenv install pytest","title":"pipenv"},{"location":"#poetry","text":"poetry add dataget","title":"poetry"},{"location":"#contributing","text":"Read our guide on Creating a Dataset if you are interested in adding a dataset to dataget.","title":"Contributing"},{"location":"#license","text":"MIT License","title":"License"},{"location":"advanced-usage/","text":"Advanced Usage \u00b6 Specifying Download Directory \u00b6 By default every dataset is downloaded inside a ./data/{dataset_name} folder in the current directory. There are two ways you can control where the data is stored: the first one is use the global_cache keyword argument on the constructor of any Dataset dataget . vision . mnist ( global_cache = True ) . get () This will download the dataset inside ~/.dataget/{dataset_name} instead, this is useful if you want to reuse the same dataset across projects. The second is to specify the exact location yourself by using the path keyword argument instead: dataget . vision . mnist ( path = \"/my/dataset/path\" ) . get () This gives you full control over the exact location, in this case the dataset will be downloaded /my/dataset/path . \u00b6","title":"Advanced Usage"},{"location":"advanced-usage/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"advanced-usage/#specifying-download-directory","text":"By default every dataset is downloaded inside a ./data/{dataset_name} folder in the current directory. There are two ways you can control where the data is stored: the first one is use the global_cache keyword argument on the constructor of any Dataset dataget . vision . mnist ( global_cache = True ) . get () This will download the dataset inside ~/.dataget/{dataset_name} instead, this is useful if you want to reuse the same dataset across projects. The second is to specify the exact location yourself by using the path keyword argument instead: dataget . vision . mnist ( path = \"/my/dataset/path\" ) . get () This gives you full control over the exact location, in this case the dataset will be downloaded /my/dataset/path .","title":"Specifying Download Directory"},{"location":"dataset/","text":"Creating a Dataset \u00b6 The Dataset class defined these 4 abstract methods which you must implement: name : a property that should return the name of the dataset e.g. vision_mnist . download : the method that should download the dataset to disk and possibly perform other tasks such as file extraction, organization, and clean_cacheup. load : the method that loads the data into memory and possibly structures it in the most convenient format for the user. Path The self.path field is a pathlib.Path that tells the dataset where the data should be stored. The get method ensures this path exists before calling download or load ; use this field when implementing these methods. get kwargs \u00b6 The get method will accept **kwargs which it will forward to download and load so all these methods have to accept the same arguments. An alternatively strategy is have each accept its desired required or optional arguments and accept **kwargs to accumulate the additional it doesn't need. For example: def download ( self , version , limit = None , ** kwargs ): # code def load ( self , dtype = np . float32 , ** kwargs ): # code With this implementation you the get method could be called like this: . get ( version = \"0.0.2\" , dtype = np . uint8 ) Note In the previous example the version argument is required because download uses it as a positional argument although for get its just a keyword argument, if it were not passed a TypeError would've been raised. Template \u00b6 You can use this template to get started. from dataget.dataset import Dataset class SomeDataset ( Dataset ): # OPTIONAL def __init__ ( self , init_arg , ** kwargs ): # code super () . __init__ ( ** kwargs ) # !!IMPORTANT @property def name ( self ): return \"some_dataset_name\" def download ( self , some_arg , ** kwargs ): # code def load ( self , other_arg , ** kwargs ): # code return data1 , data2 , data3 , ... Warning If you are definint you own __init__ remenber to always forward **kwargs to super().__init__ since its important that all datasets support the path and global_cache keyword arguments defined in the Dataset class. If super().__init__ is not called at all the path field will not be instantiated and errors will occure.","title":"Creating a Dataset"},{"location":"dataset/#creating-a-dataset","text":"The Dataset class defined these 4 abstract methods which you must implement: name : a property that should return the name of the dataset e.g. vision_mnist . download : the method that should download the dataset to disk and possibly perform other tasks such as file extraction, organization, and clean_cacheup. load : the method that loads the data into memory and possibly structures it in the most convenient format for the user. Path The self.path field is a pathlib.Path that tells the dataset where the data should be stored. The get method ensures this path exists before calling download or load ; use this field when implementing these methods.","title":"Creating a Dataset"},{"location":"dataset/#get-kwargs","text":"The get method will accept **kwargs which it will forward to download and load so all these methods have to accept the same arguments. An alternatively strategy is have each accept its desired required or optional arguments and accept **kwargs to accumulate the additional it doesn't need. For example: def download ( self , version , limit = None , ** kwargs ): # code def load ( self , dtype = np . float32 , ** kwargs ): # code With this implementation you the get method could be called like this: . get ( version = \"0.0.2\" , dtype = np . uint8 ) Note In the previous example the version argument is required because download uses it as a positional argument although for get its just a keyword argument, if it were not passed a TypeError would've been raised.","title":"get kwargs"},{"location":"dataset/#template","text":"You can use this template to get started. from dataget.dataset import Dataset class SomeDataset ( Dataset ): # OPTIONAL def __init__ ( self , init_arg , ** kwargs ): # code super () . __init__ ( ** kwargs ) # !!IMPORTANT @property def name ( self ): return \"some_dataset_name\" def download ( self , some_arg , ** kwargs ): # code def load ( self , other_arg , ** kwargs ): # code return data1 , data2 , data3 , ... Warning If you are definint you own __init__ remenber to always forward **kwargs to super().__init__ since its important that all datasets support the path and global_cache keyword arguments defined in the Dataset class. If super().__init__ is not called at all the path field will not be instantiated and errors will occure.","title":"Template"},{"location":"datasets/kaggle/","text":"dataget.kaggle \u00b6 Download any dataset from the Kaggle platform and immediately loads it into memory: import dataget df_train , df_test = dataget . kaggle ( \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) In this example we downloaded the Point Cloud Mnist 2D dataset from Kaggle and load the train.csv and test.csv files as pandas dataframes. Config To start using this Dataset make sure you have properly installed and configured the Kaggle API . Supported Formats \u00b6 Right now we only support the csv format. In the future we want to be able to load any file that numpy or pandas can read. API Reference \u00b6 kaggle \u00b6 __init__ ( self , dataset = None , competition = None , ** kwargs ) \u00b6 Show source code in kaggle.py 16 17 18 19 20 21 22 23 24 25 26 27 28 def __init__ ( self , dataset : str = None , competition : str = None , ** kwargs ): \"\"\" Arguments: dataset: the id of the kaggle dataset in the format `username/dataset_name` \"\"\" assert ( dataset is not None != competition is not None ), \"Set either dataset or competition\" self . kaggle_dataset = dataset self . kaggle_competition = competition super () . __init__ ( ** kwargs ) Parameters Name Type Description Default dataset str the id of the kaggle dataset in the format username/dataset_name None load ( self , files , ** kwargs ) \u00b6 Show source code in kaggle.py 47 48 49 50 51 52 def load ( self , files : list , ** kwargs ): \"\"\" Arguments: files: the list of files that will be loaded into memory \"\"\" return [ self . _load_file ( filename ) for filename in files ] Parameters Name Type Description Default files list the list of files that will be loaded into memory required","title":"kaggle"},{"location":"datasets/kaggle/#datagetkaggle","text":"Download any dataset from the Kaggle platform and immediately loads it into memory: import dataget df_train , df_test = dataget . kaggle ( \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) In this example we downloaded the Point Cloud Mnist 2D dataset from Kaggle and load the train.csv and test.csv files as pandas dataframes. Config To start using this Dataset make sure you have properly installed and configured the Kaggle API .","title":"dataget.kaggle"},{"location":"datasets/kaggle/#supported-formats","text":"Right now we only support the csv format. In the future we want to be able to load any file that numpy or pandas can read.","title":"Supported Formats"},{"location":"datasets/kaggle/#api-reference","text":"","title":"API Reference"},{"location":"datasets/kaggle/#dataget.kaggle.kaggle","text":"","title":"kaggle"},{"location":"datasets/kaggle/#dataget.kaggle.kaggle.__init__","text":"Show source code in kaggle.py 16 17 18 19 20 21 22 23 24 25 26 27 28 def __init__ ( self , dataset : str = None , competition : str = None , ** kwargs ): \"\"\" Arguments: dataset: the id of the kaggle dataset in the format `username/dataset_name` \"\"\" assert ( dataset is not None != competition is not None ), \"Set either dataset or competition\" self . kaggle_dataset = dataset self . kaggle_competition = competition super () . __init__ ( ** kwargs ) Parameters Name Type Description Default dataset str the id of the kaggle dataset in the format username/dataset_name None","title":"__init__()"},{"location":"datasets/kaggle/#dataget.kaggle.kaggle.load","text":"Show source code in kaggle.py 47 48 49 50 51 52 def load ( self , files : list , ** kwargs ): \"\"\" Arguments: files: the list of files that will be loaded into memory \"\"\" return [ self . _load_file ( filename ) for filename in files ] Parameters Name Type Description Default files list the list of files that will be loaded into memory required","title":"load()"},{"location":"datasets/toy/spirals/","text":"dataget.toy.spirals \u00b6 This is an artificial dataset created using polar functions inspired by a similar dataset found in tensorflow playground . import dataget df_train , df_test = dataget . toy . spirals () . get () Sample \u00b6 Format \u00b6 type shape df_train pd.DataFrame (399, 3) df_test pd.DataFrame (45, 3) Features \u00b6 column type description x0 float coordinate x1 float coordinate y int label Info \u00b6 Folder name : toy_spirals Size on disk : 24KB","title":"spirals"},{"location":"datasets/toy/spirals/#datagettoyspirals","text":"This is an artificial dataset created using polar functions inspired by a similar dataset found in tensorflow playground . import dataget df_train , df_test = dataget . toy . spirals () . get ()","title":"dataget.toy.spirals"},{"location":"datasets/toy/spirals/#sample","text":"","title":"Sample"},{"location":"datasets/toy/spirals/#format","text":"type shape df_train pd.DataFrame (399, 3) df_test pd.DataFrame (45, 3)","title":"Format"},{"location":"datasets/toy/spirals/#features","text":"column type description x0 float coordinate x1 float coordinate y int label","title":"Features"},{"location":"datasets/toy/spirals/#info","text":"Folder name : toy_spirals Size on disk : 24KB","title":"Info"},{"location":"datasets/vision/cifar10/","text":"dataget.vision.cifar10 \u00b6 Downloads the Cifar 10 dataset from University of Toronto's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . vision . cifar10 () . get () Sample \u00b6 Format \u00b6 type shape dtype X_train np.array (50_000, 32, 32, 3) uint8 y_train np.array (50_000, 1) uint8 X_test np.array (10_000, 32, 32, 3) uint8 y_test np.array (10_000, 1) uint8 Info \u00b6 Folder name : vision_cifar10 Size on disk : 178M","title":"cifar10"},{"location":"datasets/vision/cifar10/#datagetvisioncifar10","text":"Downloads the Cifar 10 dataset from University of Toronto's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . vision . cifar10 () . get ()","title":"dataget.vision.cifar10"},{"location":"datasets/vision/cifar10/#sample","text":"","title":"Sample"},{"location":"datasets/vision/cifar10/#format","text":"type shape dtype X_train np.array (50_000, 32, 32, 3) uint8 y_train np.array (50_000, 1) uint8 X_test np.array (10_000, 32, 32, 3) uint8 y_test np.array (10_000, 1) uint8","title":"Format"},{"location":"datasets/vision/cifar10/#info","text":"Folder name : vision_cifar10 Size on disk : 178M","title":"Info"},{"location":"datasets/vision/cifar100/","text":"dataget.vision.cifar100 \u00b6 Downloads the Cifar 100 dataset from University of Toronto's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . vision . cifar100 () . get () Sample \u00b6 Format \u00b6 type shape dtype X_train np.array (50_000, 32, 32, 3) uint8 y_train np.array (50_000, 1) uint8 X_test np.array (10_000, 32, 32, 3) uint8 y_test np.array (10_000, 1) uint8 Info \u00b6 Folder name : vision_cifar100 Size on disk : 178M","title":"cifar100"},{"location":"datasets/vision/cifar100/#datagetvisioncifar100","text":"Downloads the Cifar 100 dataset from University of Toronto's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . vision . cifar100 () . get ()","title":"dataget.vision.cifar100"},{"location":"datasets/vision/cifar100/#sample","text":"","title":"Sample"},{"location":"datasets/vision/cifar100/#format","text":"type shape dtype X_train np.array (50_000, 32, 32, 3) uint8 y_train np.array (50_000, 1) uint8 X_test np.array (10_000, 32, 32, 3) uint8 y_test np.array (10_000, 1) uint8","title":"Format"},{"location":"datasets/vision/cifar100/#info","text":"Folder name : vision_cifar100 Size on disk : 178M","title":"Info"},{"location":"datasets/vision/fashion_mnist/","text":"dataget.vision.fashion_mnist \u00b6 Downloads the Fashion MNIST dataset and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . vision . fashion_mnist () . get () Sample \u00b6 Format \u00b6 type shape dtype X_train np.array (60_000, 28, 28) uint8 y_train np.array (60_000,) uint8 X_test np.array (10_000, 28, 28) uint8 y_test np.array (10_000,) uint8 Info \u00b6 Folder name : vision_fashion_mnist Size on disk : 53MB","title":"fashion_mnist"},{"location":"datasets/vision/fashion_mnist/#datagetvisionfashion_mnist","text":"Downloads the Fashion MNIST dataset and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . vision . fashion_mnist () . get ()","title":"dataget.vision.fashion_mnist"},{"location":"datasets/vision/fashion_mnist/#sample","text":"","title":"Sample"},{"location":"datasets/vision/fashion_mnist/#format","text":"type shape dtype X_train np.array (60_000, 28, 28) uint8 y_train np.array (60_000,) uint8 X_test np.array (10_000, 28, 28) uint8 y_test np.array (10_000,) uint8","title":"Format"},{"location":"datasets/vision/fashion_mnist/#info","text":"Folder name : vision_fashion_mnist Size on disk : 53MB","title":"Info"},{"location":"datasets/vision/imagenet/","text":"dataget.vision.imagenet \u00b6 Downloads the ImageNet dataset from their official ImageNet Object Localization Challenge Kaggle competition and loads its metadata as pandas dataframes. You need the Kaggle CLI installed and configured to use this dataset. import dataget df_train , df_val , df_test = dataget . vision . imagenet () . get () Dataget doesn't load the images of this dataset into memory, instead the df_train , df_val , and df_test dataframes has the image_path column which contains the relative path of each sample which you can latter use to iteratively load each image during training. Sample \u00b6 Format \u00b6 type shape df_train pd.DataFrame (544_546, 10) df_val pd.DataFrame (50_000, 9) df_test pd.DataFrame (100_000, 2) Features \u00b6 column type description df_train df_val df_test ImageId str image id x x x image_path str relative path to jpeg image x x x annotations_path str relative path to pascal voc xml x x label str label id x x label_name str label name x x PredictionString str prediction string x x xmin int64 prediction string bouding box coord x x ymin int64 prediction string bouding box coord x x xmax int64 prediction string bouding box coord x x ymax int64 prediction string bouding box coord x x wnid str WordNet ID x Info \u00b6 Folder name : vision_imagenet Size on disk : 161GB","title":"imagenet"},{"location":"datasets/vision/imagenet/#datagetvisionimagenet","text":"Downloads the ImageNet dataset from their official ImageNet Object Localization Challenge Kaggle competition and loads its metadata as pandas dataframes. You need the Kaggle CLI installed and configured to use this dataset. import dataget df_train , df_val , df_test = dataget . vision . imagenet () . get () Dataget doesn't load the images of this dataset into memory, instead the df_train , df_val , and df_test dataframes has the image_path column which contains the relative path of each sample which you can latter use to iteratively load each image during training.","title":"dataget.vision.imagenet"},{"location":"datasets/vision/imagenet/#sample","text":"","title":"Sample"},{"location":"datasets/vision/imagenet/#format","text":"type shape df_train pd.DataFrame (544_546, 10) df_val pd.DataFrame (50_000, 9) df_test pd.DataFrame (100_000, 2)","title":"Format"},{"location":"datasets/vision/imagenet/#features","text":"column type description df_train df_val df_test ImageId str image id x x x image_path str relative path to jpeg image x x x annotations_path str relative path to pascal voc xml x x label str label id x x label_name str label name x x PredictionString str prediction string x x xmin int64 prediction string bouding box coord x x ymin int64 prediction string bouding box coord x x xmax int64 prediction string bouding box coord x x ymax int64 prediction string bouding box coord x x wnid str WordNet ID x","title":"Features"},{"location":"datasets/vision/imagenet/#info","text":"Folder name : vision_imagenet Size on disk : 161GB","title":"Info"},{"location":"datasets/vision/mnist/","text":"dataget.vision.mnist \u00b6 Downloads the MNIST dataset from Yann LeCun's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . vision . mnist () . get () Sample \u00b6 Format \u00b6 type shape dtype X_train np.array (60_000, 28, 28) uint8 y_train np.array (60_000,) uint8 X_test np.array (10_000, 28, 28) uint8 y_test np.array (10_000,) uint8 Info \u00b6 Folder name : vision_mnist Size on disk : 53MB","title":"mnist"},{"location":"datasets/vision/mnist/#datagetvisionmnist","text":"Downloads the MNIST dataset from Yann LeCun's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . vision . mnist () . get ()","title":"dataget.vision.mnist"},{"location":"datasets/vision/mnist/#sample","text":"","title":"Sample"},{"location":"datasets/vision/mnist/#format","text":"type shape dtype X_train np.array (60_000, 28, 28) uint8 y_train np.array (60_000,) uint8 X_test np.array (10_000, 28, 28) uint8 y_test np.array (10_000,) uint8","title":"Format"},{"location":"datasets/vision/mnist/#info","text":"Folder name : vision_mnist Size on disk : 53MB","title":"Info"}]}